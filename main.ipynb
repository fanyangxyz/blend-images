{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to installl LLaVA https://github.com/haotian-liu/LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "def image_parser(image_file):\n",
    "    out = image_file.split(\",\")\n",
    "    return out\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    image.resize((300, 300))\n",
    "    return image\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do this because my GPU memory is small\n",
    "if 'pipe' in locals() or 'pipe' in globals():\n",
    "    print('deleting pipe')\n",
    "    del pipe\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    print('deleting model')\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwangz/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c078b9c2879b48009ac5496316a54509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwangz/.venv/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.utils import disable_torch_init\n",
    "disable_torch_init()\n",
    "\n",
    "# the code below only works for this model\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_base = None\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, model_base, model_name, offload_folder=\"offload\", load_4bit=True\n",
    ")\n",
    "print(model.device)\n",
    "print(model.dtype)\n",
    "print('all loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
      "<image> Describe a blend of both images. ASSISTANT:\n",
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwangz/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/dwangz/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image features a unique artwork that combines elements of a painting and a sculpture. The painting consists of a series of circles, with one large circle in the center and three smaller circles surrounding it. The sculpture is represented by a hanging mobile, which is a three-dimensional structure made of metal, featuring a combination of red, blue, and yellow colors. The mobile is suspended from a string, creating a visually striking and creative piece of art. The painting and the mobile together create an interesting and captivating visual experience.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<image> Describe a blend of both images.\"\n",
    "image_file = \"/home/dwangz/Downloads/calder.png,/home/dwangz/Downloads/blue_orange_green.png\"\n",
    "image_file = \"examples/calder.png,examples.blue_orange_green.png\"\n",
    "\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    ")\n",
    "qs = prompt\n",
    "qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "conv_mode = \"llava_v1\"\n",
    "\n",
    "from llava.conversation import conv_templates\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "print(prompt)\n",
    "\n",
    "from llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    ")\n",
    "\n",
    "import torch\n",
    "image_files = image_parser(image_file)\n",
    "images = load_images(image_files)\n",
    "for image in images:\n",
    "    image.show()\n",
    "image_sizes = [x.size for x in images]\n",
    "images_tensor = process_images(\n",
    "    images,\n",
    "    image_processor,\n",
    "    model.config\n",
    ").to(model.device, dtype=torch.float16)\n",
    "\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .cuda()\n",
    ")\n",
    "\n",
    "args = type('Args', (), {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 512,\n",
    "})()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    print('generating...')\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=images_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=True if args.temperature > 0 else False,\n",
    "        temperature=args.temperature,\n",
    "        top_p=args.top_p,\n",
    "        num_beams=args.num_beams,\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting model\n"
     ]
    }
   ],
   "source": [
    "# need to do this because my GPU memory is small\n",
    "if 'pipe' in locals() or 'pipe' in globals():\n",
    "    print('deleting pipe')\n",
    "    del pipe\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    print('deleting model')\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c38b63cdf0f449ca7d97a4f42a75b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipe created\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float32)\n",
    "print('pipe created')\n",
    "pipe.to('cuda')\n",
    "print(pipe.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (106 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['suspended from a string, creating a visually striking and creative piece of art. the painting and the mobile together create an interesting and captivating visual experience.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c92797ee5e24295b1034077a2056c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = pipe(outputs).images[0]\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectname",
   "language": "python",
   "name": "projectname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
